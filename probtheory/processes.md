# Случайные процессы

Случайный процесс
-

$X(t) : T\subset\mathbb{R} \rightarrow \mathbb{R}^*$ - НЕ случайный процесс.

Обычно, $t$ - время, $T = [0,+\infty)$.

На самом деле $X$ - функция еще и от $\omega$, так что $X(\omega, t):: \Omega\times\mathbb{R}\rightarrow\mathbb{R}^*$

$\omega$ - fix, $X(t)$ - траектория (обычная вещественная функция, $X(t) :: \mathbb{R} \rightarrow\mathbb{R}$)

**Зам.** Случайный процесс - просто функция $X(\omega, t)$. 

**Зам.** Можно сказать, что это $\text{Trajectory}(\omega)$.

**Зам.** Если $|\Omega| \leqslant \aleph_1$, $\text{Сл. процесс} = X(R, t)$, где $R \in \mathbb{R}^*$. Иногда фиксируются:
- $r =$ реализация $R$ и тогда рассматрифается вещественная функция $t \mapsto X(r,t)$ - траектория
- значение $t$, тогда это случайная величина, полученная преобразованием случайной величины $R$: $Y = X(R, t_{\text{fix}}) = f(R)$

Характеристики
-
$F(\vec x, \vec t) = P(X(t_1)\leqslant x_1,..,X(t_n)\leqslant x_n)$ - функция распределения конечномерных распределений.

$m(t)=EX(t), k(s,t)=\text{Cov}(X(t), X(s))$ - матожидание и ковариация

$\widehat{k(s,t)} = EX(t)X(s)$ - корреляция


Процесс с независимыми приращениями
-

$X(t)$ - процесс с независимыми приращениями, если $\forall t_0\lt t_1\lt..\lt t_{n}$ поточечные разности функций $\Omega \rightarrow \mathbb{R}$ $$X_{t_0},X_{t_1} - X_{t_0},..,X_{t_{n}} - X_{t_{n-1}}\text{ - независимы}$$

Пуассоновский процесс
-

Есть несколько определений (пуассоновский процесс с интенсивностью $\lambda$):
1)  1) $X(0) \stackrel{\text{a.s.}}{=}0$
    2) $X(t)$ - процесс с независимыми приращениями
    3) $\forall t>s\ X(t) - X(s) = \text{Pois}(\lambda(t-s))$

2) 

**Свойства**
1) $t>s\Rightarrow P(X(\omega, t) \geqslant X(\omega, s)) = 1$
2) $X(t) = \text{Pois}(\lambda t)$

Бывает нужно считать, что $\lambda$ зависит от $t$.

**Зам.** Можно подумать, что достаточно записать простую формулу $X(t) = \text{Pois}(\lambda t)$. Но здесь этого мало: чтобы описать весь процесс, нужно объяснить связь между $X(t)$ и $X(s)$ подобно тому, как независимость случайных величин $X$, $Y$ нужно постулировать извне (CDF ничего об этом не скажут). Верно, тем не менее, что $X(t) = \text{Pois}(\lambda t)$.

Стационарность в узком смысле
-

$(X(t_1),..,X(t_n))\stackrel{\text{d}}{=}(X(t_1+\Delta t), .., X(t_n+\Delta t))$

**Свойства:**
- $m(t) = \text{const}(t)$
- $k(s,t) = k(|s-t|)$ (зависит только от разности)
- Это НЕ означает, что траектория - прямая

Стационарность в широком смысле
-

- $m(t), \text{var}(t) = \text{const}(t)$
- $k(s,t) = k(|s-t|)$ (зависит только от разности)

Из стационарности в узком смысле следует стационарность в широком смысле

Гауссовский процесс
-

Процесс называется гауссовским, если $\forall t_0<..<t_n$
$$(X(t_0),..,X(t_n)) \sim \mathcal{N}(\cdot,\cdot)$$

Задается функциями $m(t)$ и $k(s, t)$.

**Свойства:**
- У гауссовских процессов стационарность в широком смысле влечет стационарность в узком смысле. **Это просто понять:** матожидание и ковариации в ГП однозначно задают распределения, значит их равенство влечет равенство распределений.

Интересное
-

Любое распределение задается бесконечным списком моментов $EX, EX^2, EX^3...$, если они существуют.

Винеровский процесс
-

$W(t), t\geqslant 0$, это гауссовский процесс и $m(t) = 0$, $k(s,t)=\min(s, t)$

**Теор.** о равносильном определении:

1. $W(0) \stackrel{\text{a.s.}}{=} 0$
2. $W(t)$ - с нез. приращениями
3. Поточечная разность $W(t)-W(s)=\mathcal{N}(0,s-t)$

Можно в 3 пункте ввести $\sigma^2$, которое здесь = 1.

**Док-во.**
($\Rightarrow$)
1. $m(0)=\text{var}(0)=k(0,0)=0$
2. $(W(t_0),W(t_1)-W(t_0)..,W(t_n)-W(t_{n-1}))$ - гауссовский вектор, как линейное преобразование гауссовского вектора $(W(t_0),W(t_1)..,W(t_n))$.

    $\text{cov}(W(t_{k+1})-W(t_{k}),W(t_{m+1})-W(t_{m})) = k(k+1, m+1) - k(k + 1, m) - k(k, m + 1) + k(k, m) = 0$, в случае нормальных величин, это означает независимость.

3. $W(t) - W(s) = \begin{pmatrix}1&-1\end{pmatrix}\cdot\begin{pmatrix}W(t)\cr W(s)\end{pmatrix}$ - линейное преобразование $\Rightarrow$ это $\mathcal{N}(\cdot,\cdot)$. Т.к. $\begin{pmatrix}W(t)\cr W(s)\end{pmatrix}=\mathcal{N}\left(\begin{pmatrix}0\cr 0\end{pmatrix};\begin{pmatrix}t&\min(s,t)\cr\min(s,t)&s\end{pmatrix}\right)$, отсюда $\text E(W(t)-W(s)) = 0, \text{Var}(W(t)-W(s)) = s + t - 2s = t - s$

В обратную сторону проще


**Свойства:**
- $t>\tau \Rightarrow E(W(t)|\left\{W(\tau), \tau < s\right\}) = W(s)$ (это случайная величина)
- $Y(t) = W(t_0+t)-W(t_0) \stackrel{\text d}{=} W(t)$
- $Z(t) = tW(\frac{1}{t}), Z(0) = 0 \Rightarrow Z(t)$ - винеровский процесс
- $X(t)$ - гауссовский с нез. прир. и стац. приращениями $\left(W(t+\Delta t) - W(t) \stackrel{d}{=} W(\Delta t)\right)$, тогда $\exist c\in \mathbb{R}, \sigma \geqslant 0$
$$X(t) = ct + \sigma W(t)$$

Теорема о случайном блуждании
-
### Первая
Если:
$U_i = \begin{matrix}1&,p = 0.5\cr-1&,p = 0.5\end{matrix}$ - i.i.d., $S_n = \sum\limits_k U_k$, $X_{h,d}(t) = S_{\lfloor\frac{t}{h}\rfloor}$

Тогда, если $d = \sqrt h$ и $X_h(t) = \sqrt hS_{\lfloor\frac{t}{h}\rfloor}$, то
$$X_{h}(t)\stackrel{\text d}{\rightarrow} W(t), \text{ при }{h\rightarrow 0}$$

### Вторая
$U_i$ - i.i.d, -//-, $W_n(t) = \sum\limits_{k=1}^{\lfloor nt\rfloor}U_k$, тогда
$$W_n(t)\stackrel{\text d}{\rightarrow} W(t)$$

Еще гауссовские процессы
-

Броуновский мост: $B(t), t\in(0,1)$. $m(t) = 0, k(s, t) = \min(s,t)-st$

Процесс Ориштейна-Уленбека: $m(t) = 0, k(s,t)=\exp(\frac{|s-t|}{2})$

Процесс Андерсон-Дарлинга: $m(t) = 0, k(s,t) = \frac{\min(t, s) - st}{\sqrt{t(1-t)s(1-s)}}$





